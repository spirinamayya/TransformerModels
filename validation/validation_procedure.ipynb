{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rectools import Columns, ExternalIds\n",
    "import tqdm\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from lightning_fabric import seed_everything\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "import typing as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rectools.model_selection import TimeRangeSplitter, LastNSplitter, cross_validate\n",
    "from rectools.dataset import Interactions\n",
    "from rectools.models import model_from_params\n",
    "from rectools.models.nn.transformers.sasrec import SASRecModel\n",
    "from rectools.models.nn.transformers.bert4rec import BERT4RecModel\n",
    "from rectools.dataset import Dataset\n",
    "from rectools.metrics import (\n",
    "    calc_metrics,\n",
    "    NDCG,\n",
    "    AvgRecPopularity,\n",
    "    CatalogCoverage,\n",
    "    Recall,\n",
    "    Serendipity,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MovieLens 1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\n",
    "        \"../Datasets/ratings.dat\",\n",
    "        sep=\"::\",\n",
    "        names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"],\n",
    "        engine=\"python\",\n",
    "    )\n",
    "\n",
    "ratings = ratings[ratings[\"rating\"] >= 1].drop(columns=[\"rating\"])\n",
    "ratings.rename(columns={\n",
    "        \"userId\": Columns.User,\n",
    "        \"movieId\": Columns.Item,\n",
    "        \"timestamp\": Columns.Datetime,\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "ratings[Columns.Datetime] = pd.to_datetime(ratings[Columns.Datetime], unit=\"s\")\n",
    "ratings[Columns.Weight] = 1\n",
    "\n",
    "TEST_SPLIT_SIZE = \"7D\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MovieLens 20M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\n",
    "        \"../Datasets/ratings.csv\",\n",
    "    )\n",
    "\n",
    "ratings = ratings[ratings[\"rating\"] >= 0].drop(columns=[\"rating\"])\n",
    "ratings.rename(columns={\n",
    "        \"userId\": Columns.User,\n",
    "        \"movieId\": Columns.Item,\n",
    "        \"timestamp\": Columns.Datetime,\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "ratings[Columns.Datetime] = pd.to_datetime(ratings[Columns.Datetime], unit=\"s\")\n",
    "ratings[Columns.Weight] = 1\n",
    "\n",
    "TEST_SPLIT_SIZE = \"60D\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\n",
    "        \"../Datasets/interactions.csv\",\n",
    "    )\n",
    "\n",
    "ratings.rename(columns={\n",
    "        \"last_watch_dt\": Columns.Datetime,\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "ratings[Columns.Weight] = 1\n",
    "ratings = ratings[Columns.Interactions]\n",
    "\n",
    "TEST_SPLIT_SIZE = \"14D\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-based split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = TimeRangeSplitter(\n",
    "    test_size=TEST_SPLIT_SIZE,\n",
    "    n_splits=1, # for cross-validation choose more splits\n",
    "    filter_cold_users=True,\n",
    "    filter_cold_items=True,\n",
    "    filter_already_seen=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-one-out split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = LastNSplitter(\n",
    "    n=1,\n",
    "    n_splits=1, # for cross-validation choose more splits\n",
    "    filter_cold_users=True,\n",
    "    filter_cold_items=True,\n",
    "    filter_already_seen=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_iterator = splitter.split(Interactions(ratings))\n",
    "train_ids, test_ids, _ = next(iter(split_iterator))\n",
    "train = ratings.iloc[train_ids]\n",
    "test = ratings.iloc[test_ids]\n",
    "train_dataset = Dataset.construct(ratings)\n",
    "test_users = test[Columns.User].unique()\n",
    "catalog=train[Columns.Item].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modification comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "torch.use_deterministic_algorithms(True)\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "# Enable deterministic behaviour with CUDA >= 10.2\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine model parameters\n",
    "\n",
    "Here are options that should be chosen for each modification to work:\n",
    "\n",
    "### Training objective\n",
    "* Shifted sequence: choose model `SASRecModel`\n",
    "    * data_preparator_type: rectools.models.nn.transformers.sasrec.SASRecDataPreparator\n",
    "* MLM: choose model `BERT4RecModel`\n",
    "    * data_preparator_type: rectools.models.nn.transformers.bert4rec.BERT4RecDataPreparator\n",
    "* All action: choose model `BERT4RecModel`\n",
    "    * data_preparator_type: modifications.objectives.all_action.AllActionDataPreparator\n",
    "    * lightning_module_type: modifications.objectives.all_action.AllActionLightningModule\n",
    "    * backbone_type: modifications.objectives.all_action.AllActionTransformerTorchBackbone\n",
    "* Dense all action: choose model `SASRecModel`\n",
    "    * data_preparator_type: modifications.objectives.dense_all_action.DenseAllActionDataPreparator\n",
    "\n",
    "### Transformer layers\n",
    "* SASRec: \n",
    "    * transformer_layers_type: rectools.models.nn.transformers.sasrec.SASRecTransformerLayers\n",
    "* BERT4Rec: \n",
    "    * transformer_layers_type: rectools.models.nn.transformers.net_blocks.PreLNTransformerLayers\n",
    "* ALBERT: \n",
    "    * transformer_layers_type: src.models.transformers.transformer_layers.albert.AlbertLayers\n",
    "\n",
    "### Loss functions\n",
    "* softmax:\n",
    "    * loss: softmax\n",
    "* bce:\n",
    "    * loss: BCE\n",
    "* gBCE:\n",
    "    * loss: gBCE\n",
    "* sampled softmax:\n",
    "    * loss: sampled_softmax\n",
    "\n",
    "### Negative sampling\n",
    "* Sample uniformly from catalogue\n",
    "    * negative_sampler_type: rectools.models.nn.transformers.negative_sampler.CatalogUniformSampler\n",
    "* Sample uniformly from batch\n",
    "    * negative_sampler_type: modifications.negative_sampling.in_batch.InBatchSampler\n",
    "    * lightning_module_type: modifications.negative_sampling.in_batch.LogQLightningModule\n",
    "* Sample mixed negatives from catalogue and batch in certain proportion\n",
    "    * negative_sampler_type: modifications.negative_sampling.mixed.MixedSampler\n",
    "    * negative_sampler_kwargs.ratio: 0.4 (change to any float in range [0, 1])\n",
    "\n",
    "### Scoring\n",
    "* Dot product\n",
    "    * similarity_module_type: rectools.models.nn.transformers.similarity.DistanceSimilarityModule\n",
    "* Cosine\n",
    "    * similarity_module_type: rectools.models.nn.transformers.similarity.DistanceSimilarityModule\n",
    "    * similarity_module_kwargs.distance: cosine\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define trainer\n",
    "\n",
    "Specify callback for early stopping (validation loss/recsys metric), specify validation mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestModelLoad(Callback):\n",
    "\n",
    "    def __init__(self, ckpt_path) -> None:\n",
    "        self.ckpt_path = ckpt_path + \".ckpt\"\n",
    "\n",
    "    def on_fit_end(self, trainer, pl_module) -> None:\n",
    "        log_dir = trainer.log_dir\n",
    "        ckpt_path = Path(log_dir) / \"checkpoints\" / self.ckpt_path\n",
    "        checkpoint = torch.load(ckpt_path, weights_only=False)\n",
    "        pl_module.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        self.ckpt_full_path = str(ckpt_path) \n",
    "\n",
    "def get_trainer_func() -> Trainer:\n",
    "    min_val_loss_ckpt = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        filename=\"best_val_loss\",\n",
    "    )\n",
    "    early_stopping_val_loss = EarlyStopping(\n",
    "        monitor=f\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        patience=20,\n",
    "        divergence_threshold=None,\n",
    "    )\n",
    "    best_model_load = BestModelLoad(\"best_val_loss\")\n",
    "    callbacks = [\n",
    "        min_val_loss_ckpt,\n",
    "        best_model_load,\n",
    "        early_stopping_val_loss,\n",
    "    ]\n",
    "    return Trainer(\n",
    "        max_epochs=5, # set to required value\n",
    "        deterministic=True,\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True,\n",
    "        logger=CSVLogger(\"test_logs\"),\n",
    "        callbacks=callbacks,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_out_mask_for_users(\n",
    "    train, val_users\n",
    ") -> np.ndarray:\n",
    "    rank = (\n",
    "        train.sort_values(Columns.Datetime, ascending=False, kind=\"stable\")\n",
    "        .groupby(Columns.User, sort=False)\n",
    "        .cumcount()\n",
    "    )\n",
    "    val_mask = (train[Columns.User].isin(val_users)) & (rank == 0)\n",
    "    return val_mask.values\n",
    "\n",
    "def get_val_mask_func(train: pd.DataFrame) -> np.ndarray:\n",
    "    users = train[Columns.User].unique()\n",
    "    val_users = users[:2048]\n",
    "    return leave_one_out_mask_for_users(train, val_users=val_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "example_model_parameters = {\n",
    "    \"cls\": SASRecModel,\n",
    "    \"loss\": \"sampled_softmax\",\n",
    "    \"transformer_layers_type\": \n",
    "    \"rectools.models.nn.transformers.sasrec.SASRecTransformerLayers\",\n",
    "    \"negative_sampler_type\": \"modifications.negative_sampling.mixed.MixedSampler\",\n",
    "    \"negative_sampler_kwargs.ratio\": 0.4,\n",
    "    \"get_trainer_func\": get_trainer_func,\n",
    "    \"get_val_mask_func\": get_val_mask_func,\n",
    "}\n",
    "model = model_from_params(example_model_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = OrderedDict(\n",
    "    [\n",
    "        (f\"recall@{10}\", Recall(k=10)),\n",
    "        (f\"ndcg@{10}\", NDCG(k=10, divide_by_achievable=True)),\n",
    "        (f\"arp@{10}\", AvgRecPopularity(k=10, normalize=True)),\n",
    "        (f\"serendipity@{10}\", Serendipity(k=10)),\n",
    "        (f\"coverage@{10}\", CatalogCoverage(k=10, normalize=True)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute cross-validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mayyaspirina/Desktop/vkrgrid/repo/prepare_vkr/.venv/lib/python3.10/site-packages/rectools/dataset/identifiers.py:60: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unq_values = pd.unique(values)\n",
      "/Users/mayyaspirina/Desktop/vkrgrid/repo/prepare_vkr/.venv/lib/python3.10/site-packages/rectools/models/nn/item_net.py:134: UserWarning: Ignoring `CatFeaturesItemNet` block because dataset doesn't contain item features.\n",
      "  warnings.warn(explanation)\n",
      "/Users/mayyaspirina/Desktop/vkrgrid/repo/prepare_vkr/.venv/lib/python3.10/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [input_value=('rectools.models.nn.item...net.CatFeaturesItemNet'), input_type=tuple])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\n",
      "  | Name        | Type                     | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 1.7 M  | train\n",
      "-----------------------------------------------------------------\n",
      "1.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 M     Total params\n",
      "6.959     Total estimated model params size (MB)\n",
      "35        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mayyaspirina/Desktop/vkrgrid/repo/prepare_vkr/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mayyaspirina/Desktop/vkrgrid/repo/prepare_vkr/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/Users/mayyaspirina/Desktop/vkrgrid/repo/prepare_vkr/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (48) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 48/48 [00:04<00:00, 10.79it/s, v_num=4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 48/48 [00:04<00:00, 10.70it/s, v_num=4]\n"
     ]
    }
   ],
   "source": [
    "cv_results = cross_validate(\n",
    "        dataset=train_dataset,\n",
    "        splitter=splitter, # to split train data into actual train and validation\n",
    "        metrics=metrics,\n",
    "        models={\"model\": model},\n",
    "        k=10,\n",
    "        filter_viewed=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_results = (\n",
    "    pd.DataFrame(cv_results[\"metrics\"])\n",
    "    .drop(columns=[\"i_split\", \"model\"])\n",
    "    .mean()\n",
    "    .to_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mayyaspirina/Desktop/vkrgrid/repo/prepare_vkr/.venv/lib/python3.10/site-packages/rectools/dataset/identifiers.py:60: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  unq_values = pd.unique(values)\n",
      "/Users/mayyaspirina/Desktop/vkrgrid/repo/prepare_vkr/.venv/lib/python3.10/site-packages/rectools/models/nn/item_net.py:134: UserWarning: Ignoring `CatFeaturesItemNet` block because dataset doesn't contain item features.\n",
      "  warnings.warn(explanation)\n",
      "/Users/mayyaspirina/Desktop/vkrgrid/repo/prepare_vkr/.venv/lib/python3.10/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `str` - serialized value may not be as expected [input_value=('rectools.models.nn.item...net.CatFeaturesItemNet'), input_type=tuple])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\n",
      "  | Name        | Type                     | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | torch_model | TransformerTorchBackbone | 1.7 M  | train\n",
      "-----------------------------------------------------------------\n",
      "1.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 M     Total params\n",
      "6.959     Total estimated model params size (MB)\n",
      "35        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mayyaspirina/Desktop/vkrgrid/repo/prepare_vkr/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/Users/mayyaspirina/Desktop/vkrgrid/repo/prepare_vkr/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/Users/mayyaspirina/Desktop/vkrgrid/repo/prepare_vkr/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (48) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 48/48 [00:04<00:00, 10.89it/s, v_num=6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 48/48 [00:04<00:00, 10.79it/s, v_num=6]\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset)\n",
    "reco = model.recommend(\n",
    "        users=test_users,\n",
    "        dataset=train_dataset,\n",
    "        k=10,\n",
    "        filter_viewed=True,\n",
    "        on_unsupported_targets=\"warn\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_results = calc_metrics(\n",
    "    metrics=metrics,\n",
    "    reco=reco,\n",
    "    interactions=test,\n",
    "    prev_interactions=train,\n",
    "    catalog=train[Columns.Item].unique(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(metric_results, orient=\"index\").to_csv(\"metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark_kernel",
   "language": "python",
   "name": "benchmark_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
